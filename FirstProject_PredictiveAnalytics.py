# -*- coding: utf-8 -*-
"""FirstProject_PredictiveAnalytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qbUnomTI5cUh0DolRtTKijRrzS3J4djP

# First Project: Predictive Analytics
**by Abimanyu Sri Setyo**

**Project Criteria**
* The project is the result of your own work.
* The project has never been used for Machine Learning class submission in Dicoding and has not been published on any platform.
* The dataset used is quantitative data (minimum 500 data samples).
* Provide documentation using text cells in notebooks (.ipynb) to explain each stage of the project.
* Determine the solution to the problem using a machine learning or deep learning approach by selecting one of the following solutions:
  * Classification
  * Regression
  * Clustering
  * Time series and forecasting
* Draft a machine learning project report that explains the flow of your project from the selection of problem domains (problem domains), data understanding, data preparation, modeling, to the evaluation stage.

**About Project**<br>
World Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States and other developed countries are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. This research intends to pinpoint the most relevant/risk factors of heart disease, as well as predict the overall risk using logistic regression.

**About Dataset**<br>
The dataset is publically available on the [Kaggle](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression) website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD). The dataset provides the patientsâ€™ information. It includes over 4,000 records and 15 attributes.

## Table of Contents

>[First Project: Predictive Analytics](#scrollTo=oVkd8OaESZmF)

>>[Table of Contents](#scrollTo=wYT6S5r6X8Zd)

>>[Data Loading](#scrollTo=gGlozs7XMUR4)

>>>[Import Libraries](#scrollTo=Y15PjaxcNaR5)

>>>[Import Dataset](#scrollTo=YAPeViMJNg9x)

>>[Exploratory Data Analysis](#scrollTo=09MT7xZ_MYz6)

>>>[Variable Description](#scrollTo=vd_c03NnNAZU)

>>>[Drop unneeded columns](#scrollTo=VwK07vc8-Cwg)

>>>[Handle missing values](#scrollTo=qwYh6npMNDT0)

>>>[Univariate Analysis](#scrollTo=cK5yi4dNOX9k)

>>>[Multivariate Analysis](#scrollTo=waRBmgKORUyk)

>>[Data Preparation](#scrollTo=ofVVDHdNSLhY)

>>>[Train-Test-Split](#scrollTo=jCqqdYseTiwa)

>>>[Standarisasi](#scrollTo=6KPtlArFT4Zq)

>>[Model Development](#scrollTo=KNzlcAnVWsxL)

>>>[Model Development using Logistic Regression](#scrollTo=vc5jo7DDWzvP)

>>>[Model Development using Random Forest](#scrollTo=88XbFKceuspb)

>>[Testing the Model](#scrollTo=7F2DA_iS4aYu)

>>[Model Evaluation](#scrollTo=oxLz0rFzvGgK)

>>>[LogisticRegression Model](#scrollTo=UwiIQ8wj5H6j)

>>>[Random Forest Model](#scrollTo=4s9dOlqd5N-8)

>>>[Compare and get the best model](#scrollTo=Uf5S2TR_Mdeh)

>>>[MSE of the Model](#scrollTo=TZGKEf_5MUXx)

## Data Loading
Preparing the dataset for use

### Import Libraries
Import the required libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import zipfile
import os
import glob 
import warnings


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestRegressor

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
from sklearn.metrics import log_loss

import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

"""### Import Dataset
Importing datasets, here the datasets used are sourced from [Kaggle](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression), so installation of the [Kaggle library](https://pypi.org/project/kaggle/) is required.
"""

! pip install kaggle

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download dileep070/heart-disease-prediction-using-logistic-regression

local_zip = '/content/heart-disease-prediction-using-logistic-regression.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/heart-disease')
zip_ref.close()

# Commented out IPython magic to ensure Python compatibility.
# %cd heart-disease
# %ls

data = pd.read_csv("/content/heart-disease/framingham.csv")
data.head()

"""## Exploratory Data Analysis
Quoted from [algorit.ma](https://algorit.ma/blog/exploratory-data-analysis-2022/), Exploratory Data Analysis covers the critical process of preliminary investigation tests on data to identify patterns, find anomalies, test hypotheses, and check assumptions through summary statistics and graphical (visual) representations.

### Variable Description
Take an in-depth look at what's interesting to see in the dataset.
"""

data.head()

data.describe()

"""### Drop unneeded columns
Drop "education" column because that doesn't affect prediction
"""

# Check the data size to ensure rows before execution
data.shape

data = data.drop('education', axis='columns')

# Recheck the data size to make sure the row has been dropped in this execution
data.shape

"""### Handle missing values
According to [DQLab.id](https://www.dqlab.id/kursus-belajar-data-mengenal-apa-itu-missing-value), missing values will make the data unusable, and it's a shame to throw away important information in many rows just because of 1-2 missing values, so one of the right steps is to fill in the missing values. Filling in the data using the median will help "neutralize" the missing data, because filling in the median will not shift or increase the variance of the data.
"""

# Check NaN values
print ("Sum of\t->\tColumn")
print ("Values")
print ("=======================")
for i in data.columns:
    print (str(data[i].isna().sum()) + "\t->\t" + i)

# View datasets based on the column with the most missing values
data.loc[(data['glucose'].isnull())]

# Check the data size to ensure rows before execution
data.shape

# Replace the NaN value in the 'cigsPerDay' column with the median
data['cigsPerDay'] = data['cigsPerDay'].fillna((data['cigsPerDay'].median()))

# Replace the NaN value in the 'BPMeds' column with the median
data['BPMeds'] = data['BPMeds'].fillna((data['BPMeds'].median()))

# Replace the NaN value in the 'totChol' column with the median
data['totChol'] = data['totChol'].fillna((data['totChol'].median()))

# Replace the NaN value in the 'BMI' column with the median
data['BMI'] = data['BMI'].fillna((data['BMI'].median()))

# Replace the NaN value in the 'glucose' column with the median
data['glucose'] = data['glucose'].fillna((data['glucose'].median()))

# Replace the NaN value in the 'heartRate' column with the median
data['heartRate'] = data['heartRate'].fillna((data['heartRate'].median()))

# Check the data size to make sure the row has been execution
data.shape

# Recheck NaN values
print ("Sum of\t->\tColumn")
print ("Values")
print ("=======================")
for i in data.columns:
    print (str(data[i].isna().sum()) + "\t->\t" + i)

"""### Univariate Analysis
Sourced from the [SanberCode Blog](https://blog.sanbercode.com/docs/materi-eda/univariate-bivariate-multivariate-analysis/), Univariate Analysis is a technique for understanding and exploring data. The prefix 'Uni' means 'one', so univariate analysis is a single feature data analysis.
"""

data.head()

# Set Categorical and Numerical Features of Dataset 
categorical_features = ['male','currentSmoker', 'prevalentStroke','prevalentHyp' , 'diabetes','TenYearCHD']
numerical_features = ['cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']

"""**Categorical Features**

Fitur male
"""

feature = categorical_features[0]
count = data[feature].value_counts()
percent = 100*data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Fitur currentSmoker"""

feature = categorical_features[1]
count = data[feature].value_counts()
percent = 100*data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Fitur prevalentStroke"""

feature = categorical_features[2]
count = data[feature].value_counts()
percent = 100*data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Fitur prevalentHyp"""

feature = categorical_features[3]
count = data[feature].value_counts()
percent = 100*data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Fitur diabetes"""

feature = categorical_features[4]
count = data[feature].value_counts()
percent = 100*data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""Fitur TenYearCHD"""

feature = categorical_features[5]
count = data[feature].value_counts()
percent = 100*data[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""**Numerical Features**"""

data[numerical_features].hist(bins=50, figsize=(20,15))
plt.show()

"""### Multivariate Analysis
Multivariate analysis is used to analyze more than 2 variables at the same time, the resulting trends can be multidimensional in nature, this analysis will help us understand which data has complex trends in attribute combinations.
"""

plt.figure(figsize=(10, 8))
correlation_matrix = data.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, linewidths=0.5)
plt.title("Correlation Matrix", size=20)

data.drop(['BPMeds', 'prevalentStroke', 'diabetes'],axis=1,inplace=True)
data.head()

plt.figure(figsize=(10, 8))
correlation_matrix = data.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, linewidths=0.5)
plt.title("Correlation Matrix", size=20)

"""## Data Preparation

### Train-Test-Split
"""

X = data.drop(["TenYearCHD"], axis = 1)
y = data["TenYearCHD"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""### Standarisasi"""

scaler_train = StandardScaler()
scaler_train.fit(X_train[numerical_features])
X_train[numerical_features] = scaler_train.transform(X_train.loc[:, numerical_features])

scaler_test = StandardScaler()
scaler_test.fit(X_test[numerical_features])
X_test[numerical_features] = scaler_train.transform(X_test.loc[:, numerical_features])

X_train[numerical_features].describe().round(4)

"""## Model Development

### Model Development using Logistic Regression
Linear Regression is a way of modeling the problem of the relationship between an independent variable and the dependent variable.
"""

model_logRes = LogisticRegression(max_iter=1000)

model_logRes.fit(X_train,y_train)

"""### Model Development using Random Forest
Random Forest is an algorithm that combines the outputs of several decision trees to achieve one result by being formed from many trees (trees) obtained through the bagging or bootstrap aggregating process.
"""

model_RF = RandomForestRegressor(n_estimators=1000, max_depth=16, random_state=55, n_jobs=1)

model_RF.fit(X_train, y_train)

"""## Testing the Model"""

y_pred_logRes = model_logRes.predict(X_test)
y_pred_RF = model_RF.predict(X_test)

"""## Model Evaluation

### LogisticRegression Model
"""

preds_logRes=model_logRes.predict(X_test)

accuracy_logRes=(model_logRes.score(X_test,y_test).round(3))
print("Logistic Regression Accuracy\t: {:.2f}".format(accuracy_logRes))

"""### Random Forest Model"""

preds_RF=model_RF.predict(X_test)

accuracy_RF=(model_RF.score(X_test,y_test).round(3))
print("Random Forest Accuracy\t: {:.2f}".format(accuracy_RF))

"""### Compare and get the best model
Based on the testing, it can be seen that the accuracy of the Logistic Regression model has a much higher accuracy than the Random Forest. Therefore, the model chosen is Logistic Regression.
"""

model = pd.DataFrame({'Logistic Regression': [accuracy_logRes], 'Random Forest': [accuracy_RF]}, index=['Accuracy'])
model

"""### MSE of the Model"""

mse = pd.DataFrame(columns=['train', 'test'], index=['Logistic Regression','Random Forest'])
model_dict = {'Logistic Regression': model_logRes, 'Random Forest': model_RF}
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3 
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Based on the testing, it can be seen that the accuracy of the Logistic Regression model has a much higher accuracy than the Random Forest. Therefore, the model chosen is Logistic Regression."""